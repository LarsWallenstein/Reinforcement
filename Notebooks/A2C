{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A2C","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"aQy_peDhrMjY","colab_type":"text"},"source":["#GLOBAL Variables\n"]},{"cell_type":"code","metadata":{"id":"v3-downlOvYz","colab_type":"code","colab":{}},"source":["GAMMA = 0.99\n","LEARNING_RATE = 0.001\n","ENTROPY_BETA = 0.01\n","BATCH_SIZE = 128\n","NUM_ENVS = 50\n","\n","#____2_________\n","REWARD_STEPS=4\n","#____1_________\n","CLIP_GRAD = 0.1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1-EZlX7CPP8q","colab_type":"text"},"source":["1. This hyperparameter is specifying the treshold for gradient clipping, which, basically, prevents our gradients at optimization stage from becoming toolarge and pushing our policy too far. The idea is very simple if L2 norm of the gradient is bigger than some treshold we clip the gradient vector to this value\n","\n","2. How many steps ahead we'll take to approximate the total discounted reward for every action. In PG, we used around 10 steps but in A2C, we'll use our value approximation to get a state value for futhersteps, so it will be fine to decrease the number of steps"]},{"cell_type":"markdown","metadata":{"id":"oTYDRWRwrKmF","colab_type":"text"},"source":["#Imports"]},{"cell_type":"code","metadata":{"id":"9QkIzf-TRcrJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"cdc73528-1e92-49c0-d753-c491e2e5be1b","executionInfo":{"status":"ok","timestamp":1565897579789,"user_tz":-180,"elapsed":7399,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["!pip install ptan\n","!pip install tensorboardcolab\n","import gym\n","import tensorboardcolab\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ptan\n","from ptan.agent import F"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ptan in /usr/local/lib/python3.6/dist-packages (0.4)\n","Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vktHHReorEoc","colab_type":"text"},"source":["#Common file"]},{"cell_type":"code","metadata":{"id":"8AK0R-5arD5L","colab_type":"code","colab":{}},"source":["import sys\n","import time\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","class RewardTracker:\n","    def __init__(self, writer, stop_reward):\n","        self.writer = writer\n","        self.stop_reward = stop_reward\n","\n","    def __enter__(self):\n","        self.ts = time.time()\n","        self.ts_frame = 0\n","        self.total_rewards = []\n","        return self\n","\n","    def __exit__(self, *args):\n","        self.writer.close()\n","\n","    def reward(self, reward, frame, epsilon=None):\n","        self.total_rewards.append(reward)\n","        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n","        self.ts_frame = frame\n","        self.ts = time.time()\n","        mean_reward = np.mean(self.total_rewards[-100:])\n","        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n","        print(\"%d: done %d games, mean reward %.3f, speed %.2f f/s%s\" % (\n","            frame, len(self.total_rewards), mean_reward, speed, epsilon_str\n","        ))\n","        sys.stdout.flush()\n","        if epsilon is not None:\n","            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n","        self.writer.add_scalar(\"speed\", speed, frame)\n","        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n","        self.writer.add_scalar(\"reward\", reward, frame)\n","        if mean_reward > self.stop_reward:\n","            print(\"Solved in %d frames!\" % frame)\n","            return True\n","        return False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"25O7mdYYhv7S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"976b68d6-4106-4f6f-f86e-5d79d0892a95","executionInfo":{"status":"ok","timestamp":1565893629315,"user_tz":-180,"elapsed":20249,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["tbc= tensorboardcolab.TensorBoardColab()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Wait for 8 seconds...\n","TensorBoard link:\n","https://aac845df.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FQV5VPiVrSaR","colab_type":"text"},"source":["#Model"]},{"cell_type":"code","metadata":{"id":"jySsXmX8S2yx","colab_type":"code","colab":{}},"source":["class AtariA2C(nn.Module):\n","  def __init__(self, input_shape, n_actions):\n","    super(AtariA2C, self).__init__()\n","    \n","    self.conv =nn.Sequential(\n","        nn.Conv2d(input_shape[0], 32, kernal_size=8, stride = 4),\n","        nn.ReLU(),\n","        nn.Conv2d(32,64, kernel_size=4, stride=2),\n","        nn.ReLU(),\n","        nn.Conv2d(64,64, kernel_size=3, stride=1),\n","        nn.ReLU()\n","    )\n","    \n","    conv_size_out = self._get_conv_out(input_shape)\n","    self.policy = nn.Sequential(\n","        nn.Linear(conv_out_size, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, n_actions)\n","    )\n","    \n","    self.value = nn.Sequential(\n","        nn.Linear(conv_out_size, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 1)\n","    )\n","    \n","  def _get_conv_out(self, shape):\n","    o = self.conv(torch.zeros(1, *shape))\n","    \"\"\"\n","    Single asterisk (*) as used in function declaration allows variable number of arguments passed from calling\n","    environment.Inside the function it behaves as a tuple.\n","    \"\"\"\n","    \"\"\"\n","    np.prod just multiplies all the values in the array, returns a float value\n","    \"\"\"\n","    return int(np.prod(o.size()))\n","  \n","  def forward(self, x):\n","    fx = x.float()/256#normalizing\n","    conv_out = self.conv(fx).view(fx.size()[0],-1)\n","    \"\"\"\n","    View method just reshape the tensor, to the given shape, and -1 is passed, when u dono the value that will be needed\n","    \"\"\"\n","    return self.policy(conv.out), self.value(conv.out)\n","  \n","  #____unpack____________________________________________\n","  def unpack_batch(batch, net, device ='cuda'):\n","    states = []\n","    actions = []\n","    rewards = []\n","    not_done_idx = []\n","    last_states = []\n","    \n","    for idx, exp in enumerate(batch):\n","      states.append(np.array(exp.state, copy = False))\n","      actions.append(int(exp.action))\n","      rewards.append(exp.reward)\n","      if exp.last_state is not None:\n","        not_done_idx.append(idx)\n","    \n","    last_states.append(np.array(exp.last_state, copy=False))\n","    states_v = torch.FloatTensor(states).to(device)\n","    actions_v = torch.LongTensor(actions).to(device)\n","    \n","    #____last_steps________________________________________\n","    rewards_np = np.array(rewards, dtype=np.float32)\n","    if not_done_idx:\n","      last_states_v = torch.FloatTensor(last_states).to(device)\n","      last_vals_v = net(last_states_v)[1]\n","      last_vals_np = last_vals_v.data.cpu().numpy()[:,0]\n","      rewards_np[not_done_idx] +=GAMMA**REWARD_STEPS*last_vals_np\n","      \n","    ref_vals_v = torch.FloatTensor(rewards_np).to(device)\n","    return states_v, actions_t, ref_vals_v\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dh6OyPWfj4q2","colab_type":"text"},"source":["1. **Unpack_batch**: note that the reward value already contains the discounted reward for REWARD_steps ahead, as we use ptan experience soure class\n","\n","2. **last_steps**: in this block we prepare variable with the last state in our transition chain and query our network for V(s) approximation. Then this appoximation is added to the discounted reward, multiplied by gamma exponented in a number of steps"]},{"cell_type":"markdown","metadata":{"id":"9UMdWtH0rVFr","colab_type":"text"},"source":["#Training loop"]},{"cell_type":"code","metadata":{"id":"_oNaLofDoXlN","colab_type":"code","colab":{}},"source":["if __name__ == \"__main__\":\n","  device = \"cuda\"\n","  make_env = lambda:ptan.common.wrappers.wrap_dqn(gym.make(\"PongNoFrameskip-v4\"))#ПРикольно сделано\n","  envs = [make_env() for _ in range(NUM_ENVS)]                                   #Обрати внимание\n","  \n","  \n","  net = AtariA2C(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n","  print(net)\n","  \n","  agent = ptan.agent.PolicyAgent(labda x: net(x)[0], apply_softmax=True, device=device)\n","  exp_source = ptan.experience.ExperienceSourceFirstLast(envs, agent, gamma=GAMMA, steps_count = REWARD_STEPS)\n","  \n","  #______epsilon________________________________________________________\n","  optimizer = optim.Adam(net.parameters(), lr = LEARNING_RATE, eps=1e-3)\n","  #_____________________________________________________________________\n","  \n","  batch =[]\n","  \n","  with RewardTracker(writer, stop_reward =18) as tracker:\n","    ##!!!!!!!!!!!!!U ve got NO TRACKER\n","    with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:\n","      for step_idx, exp in enumerate(exp_source):\n","        batch.append(exp)\n","        \n","        #handle new rewards\n","        new_rewards = exp_source.pop_total_rewards()\n","        if new_rewards:\n","          if tracker.reward(new_rewards[0], step_idx):\n","            break\n","        if len(batch)<BATCH_SIZE:\n","          continue\n","        \n","        #COREof the method\n","        \n","        states_v, actions_v, vals_ref_v = unpack_batch(batch, net, device=device)\n","        batch.clear()\n","        optimizer.zero_grad()\n","        logits_v, value_v = net(states_v)\n","        \n","        #___value_loss___________________________________________\n","        loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)\n","        #________________________________________________________\n","        \n","        #_____Policy loss__________________________________________________\n","        log_prob_v = F.log_softmax(logits_v, dim=1)\n","        adv_v = vals_ref_v - value_v.detach()\n","        log_prob_actions_v = adv_v*log_prob_v[range(BATCH_SIZE), actions_v]\n","        loss_policy_v = - log_prob_actions_v.mean()\n","        #__________________________________________________________________\n","        \n","        #_____Entropy loss_________________________________________________\n","        prob_v = F.softmax(logits_v,dim=1)\n","        entropy_loss_v = ENTROPY_BETA*(prob_v*log_prob_v).sum(dim=1).mean()\n","        #__________________________________________________________________\n","        \n","        #____gradients_____________________________________________________\n","        loss_policy_v.backward(retrain_graph=True)\n","        grads = np.concatenate([p.grad.data.cpu().numpy().flatten() for p in net.parameters() if p.grad is not None])\n","        #__________________________________________________________________\n","        \n","        loss_v = entropy_loss_v +loss_value_v\n","        loss_v.backward()\n","        \n","        nn_utils.clip_grad_norm(net.parameters(), CLIP_GRAD)\n","        optimizer.step()\n","        loss_v+=loss_policy_v\n","        \n","        tb_tracker.track(\"advantage\",       adv_v, step_idx)\n","        tb_tracker.track(\"values\",          value_v, step_idx)\n","        tb_tracker.track(\"batch_rewards\",   vals_ref_v, step_idx)\n","        tb_tracker.track(\"loss_entropy\",    entropy_loss_v, step_idx)\n","        tb_tracker.track(\"loss_policy\",     loss_policy_v, step_idx)\n","        tb_tracker.track(\"loss_value\",      loss_value_v, step_idx)\n","        tb_tracker.track(\"loss_total\",      loss_v, step_idx)\n","        tb_tracker.track(\"grad_l2\",         np.sqrt(np.mean(np.square(grads))), step_idx)\n","        tb_tracker.track(\"grad_max\",        np.max(np.abs(grads)), step_idx)\n","        tb_tracker.track(\"grad_var\",        np.var(grads), step_idx)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJx149HrqXJm","colab_type":"text"},"source":["1. $$epsilon$$ a small value added to the denominator to prevent from division by zero, however in our case it is not small, as the algorithm showed not to converge with default values -10 or -8, noe xplanaiton is provided, but it can be, that gradient become too large\n","\n","2. $$value - loss$$   we just calculate the MSE between the value returned by our network and the approximation we performed using the bellman equation unrolled four steps forward\n","\n","3. $$Policy-loss$$       The first two steps are to obtain a log of our policy and calculate the advantage of actions, which is \n","$$A(s,a) = Q(s,a)-V(s)$$\n","The call to value_v.detach() is importnat, as we don't want to propogate the PG into our value approximation head. Then we take the log probability for the actions taken and scle them with advantage. Our Pg loss value will be equal to the negated mean of this scaled log of policy, as PG directs us toward policy improvement, but loss value is supposed to be minimized\n","\n","\n","4. $$Entropy loss$$     The last piece of our loss function is entropy loss, whish equals to the scaled entropy of our policy, taken with the oposite sign\n","\n","$$H(\\pi)=-\\sum\\pi\\log\\pi$$\n","\n","5. $$Gradients$$\n","In this block we calculate and extract gradients of our policy, which will be used to track the maximum gradient, its variance and L2 norm"]},{"cell_type":"code","metadata":{"id":"jztW3vGJq1-Z","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}