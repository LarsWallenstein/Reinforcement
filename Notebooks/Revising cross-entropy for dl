{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Revising cross-entropy for dl","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"SscEW-GfDTRk","colab_type":"code","colab":{}},"source":["import gym\n","from collections import namedtuple\n","import numpy as np\n","from tensorboardX import SummaryWriter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkXPKi9rH5_6","colab_type":"code","colab":{}},"source":["!pip install tensorboard\n","!pip install tensorboard-pytorch\n","!pip install tensorboardX"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eREvvSU6IPPC","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zeH45Zw3Iu9s","colab_type":"code","colab":{}},"source":["HIDDEN_SIZE = 128\n","BATCH_SIZE = 16\n","PERCENTILE = 70"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_cj_iDd2JAhx","colab_type":"text"},"source":["#Custom neural net"]},{"cell_type":"code","metadata":{"id":"bfaStO7rI9na","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","  def __init__(self, obs_size, hidden_size, n_actions):\n","    super(Net, self).__init__()\n","    self.net = nn.Sequential(\n","    nn.Linear(obs_size, hidden_size),\n","    nn.ReLU(),\n","    nn.Linear(hidden_size, n_actions)\n","    )\n","    \n","  def forward(self, x):\n","    return self.net(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tcnQBKLYKUIz","colab_type":"text"},"source":["###Here we will define two helper classes of named tuples"]},{"cell_type":"code","metadata":{"id":"6_-x0tGyJsId","colab_type":"code","colab":{}},"source":["Episode = namedtuple('Episode', field_names = ['reward', 'steps'])\n","EpisodeStep = namedtuple('EpisodeStep', field_names = ['observation','action'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DvOChy-XKzTR","colab_type":"text"},"source":["1. **Episode** is a single episode stored as total discounted reward and a collection of Episode step\n","\n","2. **EpisodeStep**: this will be used to represent one single step that our agent made in the episode, and it stores the oservation from the env and what action the agent completed. We'll use episode steps from elite episodes as training data"]},{"cell_type":"code","metadata":{"id":"hwVOuix7Kw6Z","colab_type":"code","colab":{}},"source":["def iterate_batches(env, net, batch_size):\n","    batch = []\n","    episode_reward = 0.0\n","    episode_steps = []\n","    obs = env.reset()\n","    sm = nn.Softmax(dim=1)\n","    while True:\n","        obs_v = torch.FloatTensor([obs])\n","        act_probs_v = sm(net(obs_v))\n","        act_probs = act_probs_v.data.numpy()[0]\n","        action = np.random.choice(len(act_probs), p=act_probs)\n","        next_obs, reward, is_done, _ = env.step(action)\n","        episode_reward += reward\n","        episode_steps.append(EpisodeStep(observation=obs, action=action))\n","        if is_done:\n","            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n","            episode_reward = 0.0\n","            episode_steps = []\n","            next_obs = env.reset()\n","            if len(batch) == batch_size:\n","                yield batch\n","                batch = []\n","        obs = next_obs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AKAh3OcqN2iF","colab_type":"text"},"source":["In the above cell we are running random choices based on the probabilities of the model and filling the two tuples initialized above, then, when episode is done we append it to the episode tuple an return a batch and when the next batch will be called itwill be yield\n","\n","\n","In the following section we are filtering episodes, selecting top 30 percent of them sorted by reward"]},{"cell_type":"code","metadata":{"id":"ZF6RlDvzNs79","colab_type":"code","colab":{}},"source":["def filter_batch(batch, percentile):\n","  rewards = list(map(lambda s: s.reward, batch))#making a list out of rewards\n","  reward_bound = np.percentile(rewards, percentile)\n","  reward_mean = float(np.mean(rewards))\n","  \n","  train_obs = []\n","  train_act = []\n","  for example in batch:\n","    if example.reward < reward_bound:\n","      continue\n","    train_obs.extend(map(lambda step: step.observation, example.steps))\n","    train_act.extend(map(lambda step: step.action, example.steps))\n","    \n","  train_obs_v = torch.FloatTensor(train_obs)\n","  train_act_v = torch.LongTensor(train_act)\n","  return train_obs_v, train_act_v, reward_bound, reward_mean"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQrvBTSFQEvk","colab_type":"text"},"source":["###Main function"]},{"cell_type":"code","metadata":{"id":"e13daTkDP_xH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2ca75344-f92c-47a6-8b3c-98779a9b69c8","executionInfo":{"status":"ok","timestamp":1564782421159,"user_tz":-180,"elapsed":16943,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["if __name__ == \"__main__\":\n","  env = gym.make(\"CartPole-v0\")\n","  #env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n","  obs_size = env.observation_space.shape[0]\n","  n_actions = env.action_space.n\n","  \n","  net = Net(obs_size, HIDDEN_SIZE, n_actions)\n","  objective = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n","  writer = SummaryWriter(comment = \"-cartpole\")\n","  \n","  for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n","    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n","    optimizer.zero_grad()\n","    action_scores_v = net(obs_v)\n","    loss_v = objective(action_scores_v, acts_v)\n","    loss_v.backward()\n","    optimizer.step()\n","    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n","            iter_no, loss_v.item(), reward_m, reward_b))\n","    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n","    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n","    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n","    if reward_m > 199:\n","      print(\"Solved!\")\n","      break\n","writer.close()"],"execution_count":23,"outputs":[{"output_type":"stream","text":["0: loss=0.690, reward_mean=18.1, reward_bound=22.0\n","1: loss=0.673, reward_mean=23.7, reward_bound=28.0\n","2: loss=0.662, reward_mean=25.8, reward_bound=27.0\n","3: loss=0.641, reward_mean=33.1, reward_bound=38.0\n","4: loss=0.644, reward_mean=32.4, reward_bound=36.0\n","5: loss=0.634, reward_mean=44.9, reward_bound=56.0\n","6: loss=0.625, reward_mean=36.0, reward_bound=45.0\n","7: loss=0.628, reward_mean=42.2, reward_bound=42.5\n","8: loss=0.623, reward_mean=46.9, reward_bound=57.0\n","9: loss=0.626, reward_mean=51.2, reward_bound=51.0\n","10: loss=0.599, reward_mean=44.7, reward_bound=55.0\n","11: loss=0.603, reward_mean=64.2, reward_bound=77.0\n","12: loss=0.583, reward_mean=48.8, reward_bound=55.5\n","13: loss=0.595, reward_mean=53.6, reward_bound=58.5\n","14: loss=0.601, reward_mean=65.6, reward_bound=85.0\n","15: loss=0.579, reward_mean=57.7, reward_bound=65.0\n","16: loss=0.579, reward_mean=58.8, reward_bound=66.5\n","17: loss=0.582, reward_mean=72.0, reward_bound=87.5\n","18: loss=0.565, reward_mean=72.0, reward_bound=89.0\n","19: loss=0.570, reward_mean=71.0, reward_bound=81.0\n","20: loss=0.567, reward_mean=70.2, reward_bound=78.5\n","21: loss=0.553, reward_mean=79.8, reward_bound=91.0\n","22: loss=0.573, reward_mean=71.4, reward_bound=73.0\n","23: loss=0.556, reward_mean=67.4, reward_bound=69.0\n","24: loss=0.550, reward_mean=92.4, reward_bound=95.0\n","25: loss=0.555, reward_mean=90.2, reward_bound=96.5\n","26: loss=0.547, reward_mean=96.1, reward_bound=104.5\n","27: loss=0.554, reward_mean=84.8, reward_bound=103.5\n","28: loss=0.553, reward_mean=111.8, reward_bound=124.0\n","29: loss=0.554, reward_mean=119.6, reward_bound=146.5\n","30: loss=0.547, reward_mean=129.2, reward_bound=169.5\n","31: loss=0.545, reward_mean=119.4, reward_bound=151.5\n","32: loss=0.544, reward_mean=117.0, reward_bound=138.5\n","33: loss=0.545, reward_mean=116.8, reward_bound=136.0\n","34: loss=0.544, reward_mean=166.2, reward_bound=200.0\n","35: loss=0.527, reward_mean=130.9, reward_bound=170.5\n","36: loss=0.522, reward_mean=141.6, reward_bound=169.5\n","37: loss=0.523, reward_mean=162.8, reward_bound=200.0\n","38: loss=0.539, reward_mean=166.7, reward_bound=196.5\n","39: loss=0.546, reward_mean=160.8, reward_bound=198.0\n","40: loss=0.520, reward_mean=162.9, reward_bound=200.0\n","41: loss=0.540, reward_mean=173.9, reward_bound=200.0\n","42: loss=0.533, reward_mean=179.3, reward_bound=200.0\n","43: loss=0.524, reward_mean=174.4, reward_bound=200.0\n","44: loss=0.528, reward_mean=190.4, reward_bound=200.0\n","45: loss=0.531, reward_mean=190.3, reward_bound=200.0\n","46: loss=0.544, reward_mean=191.7, reward_bound=200.0\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method Monitor.__del__ of <Monitor<TimeLimit<CartPoleEnv<CartPole-v0>>>>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\", line 234, in __del__\n","    self.close()\n","  File \"/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\", line 145, in close\n","    self._close_video_recorder()\n","  File \"/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\", line 217, in _close_video_recorder\n","    self.video_recorder.close()\n","  File \"/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitoring/video_recorder.py\", line 129, in close\n","    os.remove(self.path)\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/mon/openaigym.video.3.124.video000000.mp4'\n"],"name":"stderr"},{"output_type":"stream","text":["47: loss=0.536, reward_mean=196.4, reward_bound=200.0\n","48: loss=0.534, reward_mean=198.6, reward_bound=200.0\n","49: loss=0.543, reward_mean=199.7, reward_bound=200.0\n","Solved!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"svkw03XtSUno","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}