{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tabular Q-learning on frozenLake","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"bj17HoVrB4hU","colab_type":"code","colab":{}},"source":["import gym\n","import collections\n","from tensorboardX import SummaryWriter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r_8PCeSZCeUv","colab_type":"code","colab":{}},"source":["ENV_NAME =\"FrozenLake-v0\"\n","GAMMA = 0.9\n","ALPHA = 0.2\n","TEST_EPISODES =20"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wx5nb0wYCuJq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"4c7890ae-6940-4682-8112-3b7200a886f3","executionInfo":{"status":"ok","timestamp":1565032312008,"user_tz":-180,"elapsed":3927,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["class Agent:\n","  def __init__(self):\n","    self.env = gym.make(ENV_NAME)\n","    self.state = self.env.reset()\n","    self.values = collections.defaultdict(float)\n","    \n","  \"\"\"Делаем рандомное действие из текущего положения и смотри куда оно приведет и что дастб потом занесем в таблицу\"\"\"\n","  \n","  def sample_env(self):\n","    action = self.env.action_space.sample()\n","    old_state = self.state\n","    new_state, reward, is_done, _ = self.env.step(action)\n","    self.state = self.env.reset() if is_done else new_state\n","    return (old_state, action, reward, new_state)\n","  \n","  \"\"\"\n","  Пробегаемся по всем возможным действиям, а точнее их Q значениям и сравниваем их между собой тем самым \n","  отбирая наилучшее действие и наилучшее Q значение\n","  \"\"\"\n","  def best_value_and_action(self, state):\n","    best_value, best_action = None, None\n","    for action in range(self.env.action_space.n):\n","      action_value = self.values[(state, action)]\n","      if best_value is None or best_value< action_value:\n","        best_value = action_value\n","        best_action = action\n","    return best_value, best_action\n","  \n","  \"\"\"\n","  Cначала получаем лучшее Q value для следующего состояния в которое мы попадаем\n","  Затем высчитываем по формуле какое должно было бы быть новое значение Q  если бы мы просто заменяли\n","  Вычисляем взвешенную сумму между старым значением Q и новым\n","  \"\"\"\n","  def value_update(self, s, a, r, next_s):\n","    best_v, _ = self.best_value_and_action(next_s)\n","    new_val = r+GAMMA*best_v\n","    old_val = self.values[(s,a)]\n","    self.values[(s,a)] = old_val*(1-ALPHA)+new_val*ALPHA\n"," \n","  def play_episode(self, env):\n","    total_reward = 0.0\n","    state = env.reset()\n","    while True:\n","      _, action = self.best_value_and_action(state)\n","      new_state, reward, is_done, _ = env.step(action)\n","      total_reward += reward\n","      if is_done:\n","        break\n","      state = new_state\n","    return total_reward\n","  \n","if __name__ == \"__main__\":\n","  test_env = gym.make(ENV_NAME)\n","  agent = Agent()\n","  \n","  iter_no = 0\n","  best_reward = 0.0\n","  while True:\n","    iter_no +=1\n","    s, a, r, next_s = agent.sample_env()\n","    agent.value_update(s, a, r, next_s)\n","  \n","    reward = 0.0\n","    for _ in range(TEST_EPISODES):\n","      reward+=agent.play_episode(test_env)\n","    reward/=TEST_EPISODES\n","    if reward > best_reward:\n","      print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n","      best_reward = reward\n","    if reward > 0.80:\n","      print(\"Solved in %d iterations!\" % iter_no)\n","      break"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Best reward updated 0.000 -> 0.150\n","Best reward updated 0.150 -> 0.200\n","Best reward updated 0.200 -> 0.250\n","Best reward updated 0.250 -> 0.300\n","Best reward updated 0.300 -> 0.650\n","Best reward updated 0.650 -> 0.700\n","Best reward updated 0.700 -> 0.750\n","Best reward updated 0.750 -> 0.800\n","Best reward updated 0.800 -> 0.900\n","Solved in 988 iterations!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JnF2pQYMJu8Z","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}