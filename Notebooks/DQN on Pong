{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN on Pong","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"iEGjHvWNCBYJ","colab_type":"text"},"source":["#Wrappers"]},{"cell_type":"code","metadata":{"id":"6WUll-wYCDJs","colab_type":"code","colab":{}},"source":["import cv2\n","import gym\n","import gym.spaces\n","import numpy as np\n","import collections"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aUPU-lveEMIH","colab_type":"text"},"source":["#####The following wrapper presses the **FIRE** button in env that require them for the game to start. In addition to pressing **FIRE**, this wraper checks for several corner cases that are present in some games"]},{"cell_type":"code","metadata":{"id":"_QsY0r1_COdZ","colab_type":"code","colab":{}},"source":["class FireResetEnv(gym.Wrapper):\n","  def __init__(self, env = None):\n","    super(FireResetEnv, self).__init__(env)\n","    assert env.get_action_meanings()[1]=='FIRE'\n","    assert len(env.unwrapped.get_action_meanings())>=3\n","    \n","  def step(self, action):\n","    return self.env.step.action()\n","  \n","  def reset(self):\n","    self.env.reset()\n","    obs, _, done, _ = self.env.step(1)\n","    if done:\n","      self.env.reset()\n","    obs, _, done, _ = self.env.step(2)\n","    if done:\n","      self.env.reset()\n","    return obs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lUBkKUhEFI65","colab_type":"text"},"source":["###This wrapper combines the reperion of actions during K frames and pixels from two consecutive frames"]},{"cell_type":"code","metadata":{"id":"SApEqUiCFJga","colab_type":"code","colab":{}},"source":["class MaxAndSkipEnv(gym.Wrapper):\n","  def __init__(self, env=None, skip=4):\n","    \"\"\"Return only every 'skips'-th frame\"\"\"\n","    super(MaxAndSkipEnv, self).__init__(env)\n","    #most recent raw observations (for max pooling across time steps)\n","    self._obs_buffer = collections.deque(maxlen=2)\n","    self._skip = skip\n","    \n","  def step(self, action):\n","    total_reward = 0.0\n","    done = None\n","    for _ in range(self._skip):\n","      obs, reward, done, info = self.env.step(action)\n","      self._obs_buffer.append(obs)\n","      total_reward += reward\n","      if done:\n","        break\n","    max_frame = np.max(np.stack(self.__obs_buffer), axis = 0)\n","    return max_frame, total_reward, done, info\n","  \n","  \n","  def _reset(self):\n","    self._obs_buffer.clear()\n","    obs = self.env.reset()\n","    self._obs_buffer.append(obs)\n","    return obs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUzhNer2HBz0","colab_type":"text"},"source":["The goal of this wrapper is to convert input observations from the emulator, which normally has a resolution of 210x160 pixels with RGB color channels, to a gray scale 84x84 image. It does ths using a colometric grayscale conversion (which is closer to human color perception than a simple averaging of color channels), resizing the image and cropping the top and botton parts of the result"]},{"cell_type":"code","metadata":{"id":"0D7DFoUhGvKS","colab_type":"code","colab":{}},"source":["class ProcessFrame84(gym.ObservationWrapper):\n","  def __init__(self, env = None):\n","    super(ProcessFame84, self).__init__(env)\n","    self.observation_space = gym.spaces.Box(low=0, high = 255, shape = (84,84,1), dtype=np.uint8)\n","    \n","  def observation(self, obs):\n","    return ProcessFrame84.process(obs)\n","  \n","  @staticmethod\n","  def process(frame):\n","    if frame.size == 210*160*3:\n","      img = np.reshape(frame, [210, 160,3]).astype(np.float32)\n","    elif frame.size == 250*160*3:\n","      img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n","    else:\n","      assert False, \"Unkown resolution.\"\n","    img = img[:,:,0]*0.299+img[:,:,1]*0.587+img[:,:,2]*0.114\n","    resized_screen = cv2.resize(img, (84, 110), interpolation = cv2.INTER_AREA)\n","    x_t = resized_screen[18:102,:]\n","    x_t = np.reshape(x_t,[84,84,1])\n","    return x_t.astype(np.uint8)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"goNAb57lJ9c1","colab_type":"text"},"source":["This class creates a stack of subsequent frames along the first dimenson nd returns them as an observation. The purpose is to give the network an idea about the dynamics of the objects, such as the speed and direction of the ball in pong or how enemies are moving. This is very important information which is not possible to obtain from a single image"]},{"cell_type":"code","metadata":{"id":"M5l9RrVmJA3g","colab_type":"code","colab":{}},"source":["class BufferWrapper(gym.ObservationWrapper):\n","  def __init__(self, env, n_steps, dtype = np.float32):\n","    super(BufferWrapper, self).__init__(env)\n","    self.dtype = dtype\n","    old_space = env.observation_space\n","    self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis = 0),\n","                                           old_space.high.repeat(n_steps, axis = 0),\n","                                           dtype = dtype)\n","    def reset(self):\n","      self.buffer = np.zeros_like(self.observation_space.low, dtype = self.dtype)\n","      return self.observation(self.env.reset())\n","    \n","    def observation(self, observation):\n","      self.buffer[:-1] = self.buffer[1:]\n","      self.buffer[-1]=observation\n","      return self.buffer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bX5a3HWAMW4L","colab_type":"text"},"source":["The next simple wrapper changes the shape of the observation from HWC to the CHW format requires by PyTorch. The input shape of the tensor has a color channel as the last dimenson, but PyTorch's convolution layers assume the color channel to be the first dimenson"]},{"cell_type":"code","metadata":{"id":"OLRHnJEtMXgg","colab_type":"code","colab":{}},"source":["class ImageToPyTorch(gym.ObservationWrapper):\n","  def __init__(self, env):\n","    super(ImageToPyTorch, self).__init__(env)\n","    old_shape = self.observation_space.shape\n","    self.observation_space = gym.spaces.Box(low = 0.0, high = 1.0, shape = (old_shape[-1], old_shape[0], old_shape[1]), dtype = np.float32)\n","    \n","    \n","  def observation(self, observation):\n","    return np.moveaxis(observation, 2, 0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XJ9Oh5qqNrJW","colab_type":"text"},"source":["This wrapper we have in the library converts observation data from bytes to floats and scales every pixel's value to range [0.0...1.0]"]},{"cell_type":"code","metadata":{"id":"stLJMTU_NqMZ","colab_type":"code","colab":{}},"source":["class ScaledFloatFrame(gym.ObservationWrapper):\n","  def observation(self, obs):\n","    return np.array(obs).astype(np.float32)/255.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-pD19otWOAm3","colab_type":"code","colab":{}},"source":["def make_env(env_name):\n","  env = gym.make(env_name)\n","  env = MaxAndSkipEnv(env)\n","  env = FireResetEnv(env)\n","  env = ProcessFrame84(env)\n","  env = ImageToPyTorch(env)\n","  env = BufferWrapper(env, 4)\n","  return ScaledFloatFrame(env)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKKcoke3QtYC","colab_type":"text"},"source":["#DQN"]},{"cell_type":"code","metadata":{"id":"H_uZ5FeXOtN2","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","\n","class DQN(nn.Module):\n","  def __init__(self, input_shape, n_actions):\n","    super(DQN, self).__init__()\n","    \n","    self.conv = nn.Sequential(\n","    nn.Conv2d(input_shape[0], 32, kernel_size = 8, stride =4),\n","    nn.ReLU(),\n","    nn.Conv2d(32, 64, kernel_size = 4, stride = 2),\n","    nn.ReLU(),\n","    nn.Conv2d(64,64, kernel_size = 3, stride =1),\n","    nn.ReLU()\n","    )\n","    conv_out_size = self._get_conv_out(input_shape)\n","    self.fc = nn.Sequential(\n","    nn.Linear(conv_out_size, 512),\n","        nn.ReLU()\n",",\n","    nn.Linear(512, n_actions)\n","    )\n","    \"\"\"\n","    PyTorch doesn't have flatten layer so the problem of transforming 3d tensor to 1d tensor is\n","    solved in the forward() function, where we can reshape our batch of 3d tensors into a batch of 1d vectors\n","    \"\"\"\n","    \n","  def _get_conv_out(self, shape):\n","    o = self.conv(torch.zeros(1, *shape))\n","    return int(np.prod(o.size()))\n","  \n","  def forward(self, x):\n","    conv_out = self.conv(x).view(x.size()[0], -1)\n","    return self.fc(conv_out)\n","  \n","  \"\"\"\n","  The final piece of the model is the forward() function, which accepts 4D input tensor (1. batch_size, 2. color channels\n","  , which is our stack of subsequent frames, while the third and forth are image dimensons). The application of transformations\n","  is done in two steps: first we apply the convolution layer to the input and then we obtain a 4d tensor output\n","  . This result is flattened to have two dimensons: a batch size and all the parameters returned by convolution for\n","  this batch entry as one long vector of numbers. This is done by view() function of the tensors, which lets\n","  one single dimenson to be a -1 argument as a wildcard for the rest of the parameters\n","  For example, if we have a tensor T of shape (2, 3, 4) which is a 3D tensor of 24 elements, we can reshape it into\n","  aD tensorwith six rows and four columns using T.view(6, 4). This operation \n","  doesn't create a new memory object or move this data in the memory , it just changs the higher-level shape of \n","  the tensor. The same result could be obtained by T.view(-1, 4) or T.view(6, -1), which is very convinient when y\n","  our tensor has a batch size in the first dimenson\n","  \"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-AmSKURSHWBZ","colab_type":"text"},"source":["#Training"]},{"cell_type":"code","metadata":{"id":"EwGqkJ9yW0gb","colab_type":"code","colab":{}},"source":["import argparse\n","import time\n","import numpy as np\n","import collections\n","\n","import torch \n","import torch.nn as nn\n","import torch.optim as optim\n","\n","#from tensorboardX import SummaryWriter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pa1B0VuwH6F8","colab_type":"code","colab":{}},"source":["DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n","MEAN_REWARD_BOUND = 19.5 # reward boundary for the last 100 episodes to stop training"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r8QkMRafIPVU","colab_type":"code","colab":{}},"source":["GAMMA = 0.99\n","BATCH_SIZE = 32\n","REPLAY_SIZE = 10000\n","REPLAY_START_SIZE = 10000\n","LEARNING_RATE = 1e-4\n","SYNC_TARGETFRAMES = 1000"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0XStirT9Il5n","colab_type":"text"},"source":["These parameters are the following\n","\n","1. Our gamma value used for bellman approximtion\n","2. The batch size sampled from the replay buffer **(BATCH_SIZE)**\n","3. The maximum capacity of the buffer **(REPLAY_SIZE)**\n","4. The count of frames we wait for before starting training to populate the replay buffer **(REPLAY_SART_SIZE)**\n","5. The learning rate for adam\n","6. Ho often to syncronize weights between training model and target model\n"]},{"cell_type":"code","metadata":{"id":"cWKSWsVXIlM3","colab_type":"code","colab":{}},"source":["EPSILON_DECAY_LAST_FRAME = 10**5 #during 100000 frames epsilon is linearly decayed to 0.02\n","EPSILON_SART = 1.0\n","EPSILON_FINAL = 0.02"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B5BTFGN0JwRB","colab_type":"code","colab":{}},"source":["Experience = collections.namedtuple('Experience', field_names = ['stte', 'action', 'reward', 'done', 'new_state'])\n","class ExpeeriencedBuffer:\n","  def __init__(self, capacity):\n","    self.buffer = collections.deque(maxlen = capacity)\n","  \n","  def __len__(self):\n","    return len(self.buffer)\n"," \n","  def append(self, experience):\n","    self.buffer.append(experience)\n","    \n","  def sample(self, batch_size):\n","    indices = np.random.choice(len(self.buffer),batch_size, replace = False)\n","    states , actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n","    return np.array(states), np.array(actions), np.array(rewards, dtype = np.float32), np.array(dones, dtype = np.uint8), np.array(next_states)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KsdjRIWeMKQ2","colab_type":"code","colab":{}},"source":["class Agent:\n","  def __init__(self, env, exp_buffer):\n","    self.env = env\n","    self.exp_buffer = exp_buffer\n","    self._reset()\n","    \n","  def _reset(self):\n","    self.state = env.reset()\n","    self.total_reward = 0.0\n","    \n","  def play_step(self, net, epsilon = 0.0, device = \"cuda\"):\n","    done_reward = None\n","    \n","    if(np.random.random() < epsilon):\n","      action = env.action_space.sample()\n","    else:\n","      state_a = np.array([self.state], copy=False)\n","      state_v = torch.tensor(state_a).to(device)\n","      q_vals_v = net(state_v)\n","      _, act_v = torch.max(q_vals_v, dim=1)\n","      action = int(act_v.item())\n","      new_state, reward, s_done, _ = self.env.step(action)\n","      self.total_reward +=reward\n","      new_state = new_state\n","\n","      exp = Experience(self.state,action, reward, is_done, new_state)\n","      self.exp_buffer.append(exp)\n","      self.state = new_state\n","      if is_done:\n","        done_eward = self.total_reward\n","        self._reset()\n","        return done_reward\n","      \n","      \n","  def calc_loss(batch, net, tgt_net, device=\"cuda\"):\n","    states, actions, rewards, dones, next_states = batch\n","    \"\"\"\n","    In arguments we pass our batch as a tuple of arrrays repacked by sample() method in the experience buffer. \n","    The first model (net) is used to calculate gradients, while the second one is used to calculate values for \n","    the next states and this calculation should not affct gradients. To achieve this we are using the **detach()**\n","    function of the pytorch tensor toprevent gradients from flowing into the target networks graph\n","    \"\"\"\n","    states_v = torch.tensor(states).to(device)\n","    next_states_v = torch.tensor(next_states).to(device)\n","    rewards_v = torch.tensor(rewards).to(device)\n","    actions_v = torch.tensor(actions).to(device)\n","    done_mask = torch.ByteTensor(dones).to(device)\n","    \n","    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n","    \"\"\"\n","    In the line above we pass observations to the first model and extract the specific Q values for the taken \n","    actions using gaather( tensor operation. The first argument to the gather() call is a dimenson index that we \n","    want to perform gathering on (i our case it's 1 corresponding to actions). The second argument is a tensor\n","    of indices of elements to be chosen. Extra unsqueeze and squeeze calls are required to fullfil the requirements \n","    of the gather functions to the index argument and to get rid of extra dimensons that we created\n","    (the index should have the same number of dimsnsons as the data we're processing).\n","    \"\"\"\n","    next_state_values = tgt_net(next_states_v).max(1)[0]\n","    \"\"\"\n","    In the above line we apply the target network to our next_state observations and calculate the maximum Q-value along \n","    the same action dimenson 1. Function max() returns both maximum values and indicies of those values\n","    which is very convinient. However in this case we're interested only in values\n","    \"\"\"\n","    \n","    next_state_values[done_mask]=0.0\n","    \"\"\"\n","    Here we make one simple but very important point if transition in the batch is from the last step in the episode \n","    then our value of the action doesn't have a discounted reward of the next state, as there is no next state to gather\n","    reward from. THis may look minor, but it's very important in practicewithout it learning will not converge\n","    \"\"\"\n","    next_state_values = next_state_value.detach()\n","    \"\"\"\n","    In this line we detach the value from it's computation graph to prevent gradients from flowing into the nn \n","    used to calculate Q approximation for next states. This is important, as without this our backpropagation\n","    of the loss will start to affect both predictions for the current state and the next state. However we do not want\n","    to touch predictions for the next state as they're used in the Bellman equation to calculate reference Q values\n","    We're using detach method of the tensor, which returns the tensor without its calculation history\n","    \n","    \"\"\"\n","    \n","    expcted_state_action_values = next_state_values*GAMMA +rewards_v\n","    return nn.MSELoss()(state_action_values, expected_state_action_values)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"62qAhEt3vJTt","colab_type":"text"},"source":["#Training loop"]},{"cell_type":"code","metadata":{"id":"9x5B-LEeOpgy","colab_type":"code","colab":{}},"source":["if __name__ = \"__main__\":\n","  parser = argparse.ArgumentParser()\n","  parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n","  parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME, help = \"Name of the environment, default=\n","                     +DEFAULT_ENV_NAME)\n","  parser.add_argument(\"--reward\", type=float, default= MEAN_REWARD_BOUND,\n","                     help=\"Mean reward boundary for stop of training, default=%.2f\"%MEAN_REWARD_BOUND)\n","  args = parser.parse_args()\n","  device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","  \n","  \n","  env = make_env(args.env)\n","  net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n","  tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n","  \"\"\"They ll be initialized with different random weights but that doesn't matter nuch as they be syncronized\"\"\"\n","  print(net)\n","  \n","  buffer = ExperienceBuffer(REPLAY_SIZE)\n","  agent = Agent(env, buffer)\n","  epsilon = EPSILON_START\n","  \n","  optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","  total_rewards = []\n","  frame_idx = 0\n","  ts_frame = 0\n","  ts = time.time()\n","  best_mean_reward = None\n","  \n","  while True:\n","    frame_idx += 1\n","    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx/EPSILON_DECAY_LAST_FRAME)\n","    \"\"\"Linearly decreasing epsion \"\"\"\n","    reward = agent.play_step(net, epsilon, device=device)\n","    if reward in not None:\n","      total_rewards.append(reward)\n","      speed = (frame_idx - ts_frame)/(time.time() - ts)\n","      ts_frame = frame_idx\n","      ts = time.time()\n","      mean_reward = np.mean(total_rewards[-100:])\n","      print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (frame_idx, len(total_rewards), \n","                                                                               mean_reward, epsilon, speed))\n","      if best_mean_reward is None or best_mean_reward < mean_reward:\n","        torch.save(net.state_dict(), args.env+\"-best.dat\")\n","        if best_mean_reward is not None:\n","          print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n","          best_mean_reward = mean_reward\n","          if mean_reward > args.reward:\n","            print(\"Solved in %d frames!\" % frame_idx)\n","            break\n","      \"\"\"\n","      Every time our mean reward for the last 100 episodes reaches a maximum, we report this and save the model parameters\n","      \"\"\"\n","    if len(buffer)<REPLAY_START_SIZE:\n","      continue\n","    if frame_idx % SYNC_TARGET_FRAMES == 0:\n","      tgt_net.load_state_dict(net.state_dict())\n","      \n","    optimizeer.zero_grad()\n","    batch = buffer.sample(BATCH_SIZE)\n","    loss_t = calc_loss(batch, net, tgt_net, device = device)\n","    loss_t.backward()\n","    optimizer.step()\n","          "],"execution_count":0,"outputs":[]}]}