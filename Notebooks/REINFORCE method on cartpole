{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"REINFORCE method on cartpole","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"dEhONy1QfcD0","colab_type":"code","colab":{}},"source":["GAMMA = 0.99\n","LEARNING_RATE = 0.01\n","EPISODES_TO_TRAIN = 4\n","#Modifications_______\n","ENTROPY_BETA = 0.01 \n","REWARD_STEPS = 10\n","#____________________"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oIjzB0HSLTCE","colab_type":"text"},"source":["###Modifications\n","1. **entropy_beta** is the scale of entropy bonus\n","\n","2. **reward_steps** how many steps ahead the bellman equation is unrolled to estimate the discounted total reward of every transition"]},{"cell_type":"code","metadata":{"id":"1Rd0nuRmfqOA","colab_type":"code","colab":{}},"source":["import gym\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o9P8cHD-f1Gy","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","#____1_____\n","class PGN(nn.Module):\n","  def __init__(self, input_size, n_actions):\n","    super(PGN, self).__init__()\n","    \n","    self.net = nn.Sequential(\n","        nn.Linear(input_size, 128),\n","        nn.ReLU(),\n","        nn.Linear(128, n_actions)\n","    )\n","    \n","  def forward(self, x):\n","    return self.net(x)\n","  \n","  \n","#_____2_____  \n","def calc_qvals(rewards):\n","  res=[]\n","  sum_r = 0.0\n","  for r in reversed(rewards):\n","    sum_r *=GAMMA\n","    sum_r +=r\n","    res.append(sum_r)\n","  return list(reversed(res))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ELQLeyHgce4","colab_type":"text"},"source":["1. Note that despite the fact that our network return probabilities, we're not applying softmax non-linearity to the output. The reason behind that we'll use the Pytorch log_softmax function to calculate the logarithm of the softmax output at once. This way of calculation is much more numericaly stable. but we need to remeber that output is not probability, but raw scores (usually called logits)\n","\n","2. This function is a bit tricky. It accepts a list of rewards for the whole episode and needs to calculate the discounted total rewards for every step. To do this efficiently, we calculate the reward from the end of the local reward list "]},{"cell_type":"code","metadata":{"id":"xX2SNXADxDni","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ecb5d307-9d79-49a6-f770-8e90331cf515","executionInfo":{"status":"ok","timestamp":1565804753702,"user_tz":-180,"elapsed":4387,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["!pip install ptan"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ptan in /usr/local/lib/python3.6/dist-packages (0.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xVmo9AxMhHJo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":350},"outputId":"576b8704-06dd-4e64-ee18-070064d2df51","executionInfo":{"status":"error","timestamp":1565804755026,"user_tz":-180,"elapsed":804,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["import ptan\n","if __name__ == \"__main__\":\n","  env = gym.make(\"CartPole-v0\")\n","  net = PGN(env.observation_space.shape[0], env.action_space.n)\n","  \n","  print(net)\n","  #_____1_____\n","  agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor, apply_softmax=True)\n","  #Modification, additional parameter of REWARD_STEPS\n","  exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count = REWARD_STEPS)\n","  optimizer = optim.Adam(net.parameters(), lr = LEARNING_RATE)\n","  \n","  total_rewards = []\n","  done_episodes = 0\n","  \n","  batch_episodes = 0\n","  cur_rewards = [] #contains local rewards for the currently played episode\n","  batch_states, batch_actions, batch_qvals = [],[], []\n","  \n","  for step_idx, exp in enumerate(exp_source):\n","    #Modifications___________________________\n","    reward_sum +=exp.reward\n","    baseline = reward_sum/(step_idx+1)\n","    #________________________________________\n","    batch_states.append(exp.state)\n","    batch_actions.append(int(exp.action))\n","    cur_rewards.append(exp.reward)\n","    \n","    \n","    if exp.last_state is None:\n","      batch_qvals.extend(calc_qvals(cur_rewards))\n","      cur_rewards.clear()\n","      batch_episodes +=1\n","    \n","    new_rewards = exp_source.pop_total_rewards()\n","    if new_rewards:\n","      done_episodes +=1\n","      reward = new_rewards[0]\n","      total_rewards.append(reward)\n","      mean_rewards =float(np.mean(total_rewards[-100:]))\n","      print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (step_idx, reward, mean_rewards, done_episodes))\n","\n","      if mean_rewards>195:\n","        print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n","        break\n","        \n","    if batch_episodes<EPISODES_TO_TRAIN:\n","      continue\n","      \n","    optimizer.zero_grad()\n","    states_v = torch.FloatTensor(batch_states)\n","    batch_actions_t = torch.LongTensor(batch_actions)\n","    batch_qvals_v = torch.FloatTensor(batch_qvals)\n","    \n","    logits_v = net(states_v)\n","    log_prob_v = F.log_softmax(logits_v, dim=1)\n","    log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n","    loss_policy_v = -log_prob_actions_v.mean()#Modified\n","    \n","    #Modification______________________________________\n","    prob_v = F.softmax(logits_v, dim=1)\n","    entropy_v = -(prob_v*log_prob_v).sum(dim=1).mean()\n","    entropy_loss_v = -ENTROPY_BETA*entropy_v\n","    loss_v = loss_policy_v+entropy_loss_v\n","    #__________________________________________________\n","    \n","    loss_v.backward()\n","    optimizer.step()\n","    \n","    batch_episodes = 0\n","    batch_states.clear()\n","    batch_actions.clear()\n","    batch_qvals.clear()\n","    "],"execution_count":5,"outputs":[{"output_type":"stream","text":["PGN(\n","  (net): Sequential(\n","    (0): Linear(in_features=4, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=2, bias=True)\n","  )\n",")\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-ab34b38c1e33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstep_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#Modifications___________________________\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#________________________________________\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'reward_sum' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"dpx3NhysjWrE","colab_type":"text"},"source":["1. Here we are using policyagent, which needs to make a desicion about actions for every observation. As our network now returns policy in the form of probabilities of the actions to select the action to take, we need to obtain the probabilities from the network and then perform random sampling from this probability distribution.\n","Preprocessor is needed to convert gym observation of type float64 to float 32, reqyuired by Pytorch\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-tRnlI1dPPS-","colab_type":"text"},"source":["#Modified version (Starting from loop)\n"]},{"cell_type":"code","metadata":{"id":"7VtZfRSBPOkb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":248},"outputId":"e875e6c0-3457-4bdd-ad82-d934fe63bd9d","executionInfo":{"status":"error","timestamp":1565806242913,"user_tz":-180,"elapsed":937,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["BATCH_SIZE=8\n","\n","if __name__ ==  \"__main__\":\n","  env = gym.make(\"CartPole-v0\")\n","  \n","  net = PGN(env.observation_space.shape[0], env.action_space.n)\n","  \n","  agent = ptan.agent.PolicyAgent(net, preprocessor = ptan.agent.float32_preprocessor, apply_softmax = True)\n","  \n","  exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n","  \n","  optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","  \n","  total_rewards = []\n","  step_rewards = []\n","  step_idx = 0\n","  done_episodes = 0\n","  reward_sum = 0.0\n","  \n","  batch_states, batch_actions, batch_scales = [], [], []\n","  \n","  for step_idx, exp in enumerate(exp_source):\n","    reward_sum +=exp.reward\n","    baseline = reward_sum/(step_idx+1)\n","    batch_states.append(exp.state)\n","    batch_actions.append(int(exp.action))\n","    batch_scales.append(exp.reward - baseline)#scaled reward\n","    \n","    new_rewards = exp_source.pop_total_rewards()\n","    if new_rewards:\n","      done_episodes +=1\n","      reward = new_rewards[0]\n","      total_rewards.append(reward)\n","      mean_rewards = float(np.mean(total_rewards[-100:]))\n","      print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (step_idx, reward, mean_rewards, done_episodes))\n","      \n","      if mean_rewards >195:\n","        print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n","        break\n","     \n","      if len(batch_states)<BATCH_SIZE:\n","        continue\n","        \n","      states_v = torch.FloatTensor(batch_states)\n","      batch_actions_t = torch.LongTensor(batch_actions)\n","      batch_scale_v = torch.FloatTensor(batch_scales)\n","      \n","      optimizer.zero_grad()\n","      logits_v = net(states_v)\n","      log_prob_v = F.log_softmax(logits_v, dim=1)\n","      log_prob_actions_v = batch_scale_v*log_prob_v[range(BATCH_SIZE), batch_actions_t]\n","      loss_policy_v = -log_prob_actions_v.mean()\n","      \n","      prob_v= F.softmax(logits_v, dim=1)\n","      entropy_v = -(prob_v*log_prob_v).sum(dim=1).mean()\n","      entropy_loss_v = -ENTROPY_BETA*entropy_v\n","      loss_v = loss_policy_v+entropy_loss_v\n","      \n","      loss_v.backward()\n","      optimizer.step()\n","      \n","      #Calculate KL-divergence\n","      new_logits_v = net(states_v)\n","      new_prob_v = F.sotmax(new_logits_v, dim=1)\n","      kl_div_v = -((new_prob_v/ prob_v).log()*prob_v).sum(dim=1).mean()\n","      \n","      grad_max = 0.0\n","      grad_means = 0.0\n","      grad_count = 0\n","      for p in net.parameters():\n","        grad_max = max(grad_max, p.grad.abs().max().item())\n","        grad_means +=(p.grad**2).mean().sqrt().item()\n","        grad_count+=1\n","      \n","      \n","      batch_states.clear()\n","      batch_actions.clear()\n","      batch_scales.clear()\n","      "],"execution_count":13,"outputs":[{"output_type":"stream","text":["10: reward:  10.00, mean_100:  10.00, episodes: 1\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-3b398868c19c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0mlogits_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mlog_prob_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0mlog_prob_actions_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_scale_v\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlog_prob_v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_actions_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0mloss_policy_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlog_prob_actions_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [8], [11]"]}]},{"cell_type":"code","metadata":{"id":"pCt5s0NdUQBt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}