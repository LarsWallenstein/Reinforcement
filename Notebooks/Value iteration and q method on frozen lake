{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Value iteration on frozen lake","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"A5w4SYK4J7yP","colab_type":"code","colab":{}},"source":["import gym\n","import collections\n","from tensorboardX import SummaryWriter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ll3jDPpKHjt","colab_type":"code","colab":{}},"source":["!pip install tensorboardX"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"enta7WZ6KUl4","colab_type":"text"},"source":["##Global Parameters"]},{"cell_type":"code","metadata":{"id":"CK51x79bKZPX","colab_type":"code","colab":{}},"source":["ENV_NAME = \"FrozenLake-v0\"\n","GAMMA = 0.9\n","TEST_EPISODES = 20"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zdHNfON5KpLT","colab_type":"text"},"source":["###In this example we ll have to deal with different data structures, the main onew are:\n","\n","1. **Reward table**: A dictionary with the composite key \"source state\"+\"action\"+\"target state\". The value is obtained from the immediate reward.\n","\n","2. **Transitions table**: A dictionary keeping counters of the experienced transitions. Th key is the composite \"state\"+\"action\" and the value is another dictionary that maps the target state into a count of times that we've seen it. For exmple, if in state 0 we execute action 1 ten times, after three times it leads us to state 4 and after seven times to state 5. Entry the key (0, 1) in the table will be a dict{4: 3, 5: 7}. We use this table to estimate the probabilities of our transitions\n","\n","3. **Value table**: A dictionary that maps a state into the calculated valur of this state"]},{"cell_type":"markdown","metadata":{"id":"MerLV2o4MZSN","colab_type":"text"},"source":["#Agent class"]},{"cell_type":"code","metadata":{"id":"INvE5F3YKlGi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"3737696a-15c3-4661-bc43-515bdb0f2d61","executionInfo":{"status":"ok","timestamp":1565019379322,"user_tz":-180,"elapsed":7975,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["class Agent:\n","  def __init__(self):\n","    self.env = gym.make(ENV_NAME)\n","    self.state = self.env.reset()\n","    self.rewards = collections.defaultdict(float)\n","    self.transits = collections.defaultdict(collections.Counter)\n","    self.values = collections.defaultdict(float)\n","    \n","    \n","  \"\"\"We play n random steps in the game just to make more data for updating our tables later\"\"\"  \n","    \n","  def play_n_random_steps(self, count):\n","    for _ in range(count):\n","      action = self.env.action_space.sample()\n","      new_state, reward, is_done, _ = self.env.step(action)\n","      self.rewards[(self.state, action, new_state)] = reward\n","      self.transits[(self.state, action)][new_state]+=1\n","      self.state = self.env.reset() if is_done else new_state\n","      \n","  \"\"\"\n","  1. We extract transition counters for the given state and action from transition table. Counters in this table\n","  have a form of dict, with target states as key and count of expirienced transitions as value. We sum all counters\n","  to obtain the total count of times we've executed the action from the state. We will use this total value later to\n","  go from an individual counter to probability.\n","  \n","  2.Then we iterate every target state that our action has landed on and calculate its contribution into the total\\\n","  action value using Bellman equation. This contribution equals to immediate reward plus discounted value for the \n","  target state. We multiply this sum to the probability of this transition and add the result to the final action\n","  value\n","  \"\"\"\n","  def calc_action_value(self, state, action):\n","    target_counts = self.transits[(state, action)]#getting a dictionary of ending states with number of times in it\n","    total = sum(target_counts.values())# getting total number of transitions got through this action from this state\n","    action_value = 0.0\n","    for tgt_state, count in target_counts.items():\n","      reward = self.rewards[(state,action, tgt_state)]\n","      action_value += (count/total)*(reward+GAMMA*self.values[tgt_state])\n","    return action_value\n","  \n","  \n","  def select_action(self, state):\n","    best_action, best_value = None, None\n","    for action in range(self.env.action_space.n):\n","      action_value = self.calc_action_value(state, action)\n","      if best_value is None or best_value< action_value:\n","        best_value = action_value\n","        best_action = action\n","    return best_action\n","  \n","  \n","  def play_episode(self, env):\n","    total_reward = 0.0\n","    state = env.reset()\n","    while True:\n","      action = self.select_action(state)\n","      new_state, reward, is_done, _= env.step(action)\n","      self.rewards[(state, action, new_state)] = reward\n","      self.transits[(state, action)][new_state] ==1\n","      total_reward += reward\n","      if is_done:\n","        break\n","      state = new_state\n","    return total_reward\n","  \n","  \"\"\"here we' ll update state values for all possible states, just running through them, calculating action values\n","  for possible actions and taking maximum action value for each state this wille it's new state value\n","  \"\"\"\n","  def value_iteration(self):\n","    for state in range(self.env.observation_space.n):\n","      state_values = [self.calc_action_value(state, action) for action in range(self.env.action_space.n)]\n","      self.values[state] = max(state_values)\n","  \n","  \n","if __name__ == \"__main__\":\n","  test_env = gym.make(ENV_NAME)\n","  agent = Agent()\n","  writer = SummaryWriter\n","  \n","  iter_no = 0\n","  best_reward = 0.0\n","  while True:\n","    iter_no +=1\n","    agent.play_n_random_steps(100)\n","    agent.value_iteration()\n","    \n","    reward = 0.0\n","    for _ in range(TEST_EPISODES):\n","      reward+= agent.play_episode(test_env)\n","    reward /=TEST_EPISODES\n","    #writer.add_scalar(\"reward\", reward, iter_no)\n","    if reward > best_reward:\n","      best_reward = reward\n","      print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n","    if reward >0.9:\n","      print(\"Solved in %d iterations!\" % iter_no)\n","      break\n","#writer.close()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Best reward updated 0.050 -> 0.050\n","Best reward updated 0.100 -> 0.100\n","Best reward updated 0.150 -> 0.150\n","Best reward updated 0.450 -> 0.450\n","Best reward updated 0.650 -> 0.650\n","Best reward updated 0.850 -> 0.850\n","Best reward updated 0.900 -> 0.900\n","Best reward updated 0.950 -> 0.950\n","Solved in 418 iterations!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7XFoeNtnley9","colab_type":"text"},"source":["#Q-Learning for frozenLake"]},{"cell_type":"code","metadata":{"id":"YsHfdpSNaUXB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"outputId":"35ee7aa8-d6dd-4e47-8ce1-866af0a7f710","executionInfo":{"status":"ok","timestamp":1565023142333,"user_tz":-180,"elapsed":6018,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["class Agent_Q:\n","  def __init__(self):\n","    self.env = gym.make(ENV_NAME)\n","    self.state = self.env.reset()\n","    self.rewards = collections.defaultdict(float)\n","    self.transits = collections.defaultdict(collections.Counter)\n","    self.values = collections.defaultdict(float)\n","    \n","    \n","  \"\"\"We play n random steps in the game just to make more data for updating our tables later\"\"\"  \n","    \n","  def play_n_random_steps(self, count):\n","    for _ in range(count):\n","      action = self.env.action_space.sample()\n","      new_state, reward, is_done, _ = self.env.step(action)\n","      self.rewards[(self.state, action, new_state)] = reward\n","      self.transits[(self.state, action)][new_state]+=1\n","      self.state = self.env.reset() if is_done else new_state\n","      \n","  def select_action(self, state):\n","    best_action, best_value = None, None\n","    for action in range(self.env.action_space.n):\n","      action_value = self.values[(state, action)]\n","      if best_value is None or best_value< action_value:\n","        best_value = action_value\n","        best_action = action\n","    return best_action\n","  \n","  \n","  def play_episode(self, env):\n","    total_reward = 0.0\n","    state = env.reset()\n","    while True:\n","      action = self.select_action(state)\n","      new_state, reward, is_done, _= env.step(action)\n","      self.rewards[(state, action, new_state)] = reward\n","      self.transits[(state, action)][new_state] ==1\n","      total_reward += reward\n","      if is_done:\n","        break\n","      state = new_state\n","    return total_reward\n","  \n","  \"\"\"\n","  For the given state and action it needs to calculate the value of this action . To calculate this value we use \n","  the Ballman equation and our counters, which allow us to approximate the probability of the target state.\n","  However in Bellman's equation we have the value of the state and now we need to calculate it differently. Before,\n","  we had it stored in the value table, so we just took it from this table. We can't do it anymore, so we have to \n","  call  the select action method, which will choose for us  the ction with the largest q value and then we can \n","  impliment another function which could calculate for usthis value of state, but select_action does almost everything\n","  we need, so we will reuse it here\n","  \"\"\"\n","  def value_iteration(self):\n","    for state in range(self.env.observation_space.n):\n","      for action in range(self.env.action_space.n):\n","        action_value = 0.0\n","        target_counts = self.transits[(state, action)]\n","        total = sum(target_counts.values())\n","        for tgt_state, count in target_counts.items():\n","          reward = self.rewards[(state, action, tgt_state)]\n","          best_action = self.select_action(tgt_state)\n","          action_value += (count/total)*(reward+GAMMA*self.values[(tgt_state, best_action)])\n","        self.values[(state, action)] = action_value\n","\n","        \n","        \n","if __name__ == \"__main__\":\n","    test_env = gym.make(ENV_NAME)\n","    agent = Agent_Q()\n","    #writer = SummaryWriter(comment=\"-q-iteration\")\n","\n","    iter_no = 0\n","    best_reward = 0.0\n","    while True:\n","        iter_no += 1\n","        agent.play_n_random_steps(100)\n","        agent.value_iteration()\n","\n","        reward = 0.0\n","        for _ in range(TEST_EPISODES):\n","            reward += agent.play_episode(test_env)\n","        reward /= TEST_EPISODES\n","        #writer.add_scalar(\"reward\", reward, iter_no)\n","        if reward > best_reward:\n","            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n","            best_reward = reward\n","        if reward > 0.90:\n","            print(\"Solved in %d iterations!\" % iter_no)\n","            break\n","    #writer.close()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Best reward updated 0.000 -> 0.150\n","Best reward updated 0.150 -> 0.200\n","Best reward updated 0.200 -> 0.250\n","Best reward updated 0.250 -> 0.350\n","Best reward updated 0.350 -> 0.450\n","Best reward updated 0.450 -> 0.550\n","Best reward updated 0.550 -> 0.600\n","Best reward updated 0.600 -> 0.700\n","Best reward updated 0.700 -> 0.750\n","Best reward updated 0.750 -> 0.800\n","Best reward updated 0.800 -> 0.850\n","Best reward updated 0.850 -> 0.900\n","Best reward updated 0.900 -> 0.950\n","Solved in 542 iterations!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m5dPrbyGpZCL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}