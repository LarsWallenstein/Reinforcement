{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Rainbow","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"n68HjoQapG7-","colab_type":"code","colab":{}},"source":["import gym\n","import ptan\n","import argparse\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.optim as optim\n","\n","from tensorboardX import SummaryWriter\n","\n","from lib import dqn_model, common\n","\n","#n-step\n","REWARD_STEPS = 2\n","\n","#priority replay\n","PRIO_REPLAY_ALPHA = 0.6\n","BETA_START = 0.4\n","BETA_FRAMES = 100000\n","\n","#C%!\n","Vmax = 10\n","Vmin = -10\n","N_ATOMS = 51\n","DELTA_Z = (Vmax-Vmin)/(N_ATOMS-1)\n","\n","#COMMON\n","################################################################################\n","HYPERPARAMS = {\n","    'pong': {\n","        'env_name':         \"PongNoFrameskip-v4\",\n","        'stop_reward':      18.0,\n","        'run_name':         'pong',\n","        'replay_size':      100000,\n","        'replay_initial':   10000,\n","        'target_net_sync':  1000,\n","        'epsilon_frames':   10**5,\n","        'epsilon_start':    1.0,\n","        'epsilon_final':    0.02,\n","        'learning_rate':    0.0001,\n","        'gamma':            0.99,\n","        'batch_size':       32\n","    }\n","}\n","\n","class RewardTracker:\n","    def __init__(self, writer, stop_reward):\n","        self.writer = writer\n","        self.stop_reward = stop_reward\n","\n","    def __enter__(self):\n","        self.ts = time.time()\n","        self.ts_frame = 0\n","        self.total_rewards = []\n","        return self\n","\n","    def __exit__(self, *args):\n","        self.writer.close()\n","\n","    def reward(self, reward, frame, epsilon=None):\n","        self.total_rewards.append(reward)\n","        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n","        self.ts_frame = frame\n","        self.ts = time.time()\n","        mean_reward = np.mean(self.total_rewards[-100:])\n","        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n","        print(\"%d: done %d games, mean reward %.3f, speed %.2f f/s%s\" % (\n","            frame, len(self.total_rewards), mean_reward, speed, epsilon_str\n","        ))\n","        sys.stdout.flush()\n","        if epsilon is not None:\n","            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n","        self.writer.add_scalar(\"speed\", speed, frame)\n","        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n","        self.writer.add_scalar(\"reward\", reward, frame)\n","        if mean_reward > self.stop_reward:\n","            print(\"Solved in %d frames!\" % frame)\n","            return True\n","        return False\n","\n","\n","class EpsilonTracker:\n","    def __init__(self, epsilon_greedy_selector, params):\n","        self.epsilon_greedy_selector = epsilon_greedy_selector\n","        self.epsilon_start = params['epsilon_start']\n","        self.epsilon_final = params['epsilon_final']\n","        self.epsilon_frames = params['epsilon_frames']\n","        self.frame(0)\n","\n","    def frame(self, frame):\n","        self.epsilon_greedy_selector.epsilon = \\\n","            max(self.epsilon_final, self.epsilon_start - frame / self.epsilon_frames)\n","\n","\n","def distr_projection(next_distr, rewards, dones, Vmin, Vmax, n_atoms, gamma):\n","    \"\"\"\n","    Perform distribution projection aka Catergorical Algorithm from the\n","    \"A Distributional Perspective on RL\" paper\n","    \"\"\"\n","    batch_size = len(rewards)\n","    proj_distr = np.zeros((batch_size, n_atoms), dtype=np.float32)\n","    delta_z = (Vmax - Vmin) / (n_atoms - 1)\n","    for atom in range(n_atoms):\n","        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards + (Vmin + atom * delta_z) * gamma))\n","        b_j = (tz_j - Vmin) / delta_z\n","        l = np.floor(b_j).astype(np.int64)\n","        u = np.ceil(b_j).astype(np.int64)\n","        eq_mask = u == l\n","        proj_distr[eq_mask, l[eq_mask]] += next_distr[eq_mask, atom]\n","        ne_mask = u != l\n","        proj_distr[ne_mask, l[ne_mask]] += next_distr[ne_mask, atom] * (u - b_j)[ne_mask]\n","        proj_distr[ne_mask, u[ne_mask]] += next_distr[ne_mask, atom] * (b_j - l)[ne_mask]\n","    if dones.any():\n","        proj_distr[dones] = 0.0\n","        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards[dones]))\n","        b_j = (tz_j - Vmin) / delta_z\n","        l = np.floor(b_j).astype(np.int64)\n","        u = np.ceil(b_j).astype(np.int64)\n","        eq_mask = u == l\n","        eq_dones = dones.copy()\n","        eq_dones[dones] = eq_mask\n","        if eq_dones.any():\n","            proj_distr[eq_dones, l[eq_mask]] = 1.0\n","        ne_mask = u != l\n","        ne_dones = dones.copy()\n","        ne_dones[dones] = ne_mask\n","        if ne_dones.any():\n","            proj_distr[ne_dones, l[ne_mask]] = (u - b_j)[ne_mask]\n","            proj_distr[ne_dones, u[ne_mask]] = (b_j - l)[ne_mask]\n","    return proj_distr\n","\n","def unpack_batch(batch):\n","    states, actions, rewards, dones, last_states = [], [], [], [], []\n","    for exp in batch:\n","        state = np.array(exp.state, copy=False)\n","        states.append(state)\n","        actions.append(exp.action)\n","        rewards.append(exp.reward)\n","        dones.append(exp.last_state is None)\n","        if exp.last_state is None:\n","            last_states.append(state)       # the result will be masked anyway\n","        else:\n","            last_states.append(np.array(exp.last_state, copy=False))\n","    return np.array(states, copy=False), np.array(actions), np.array(rewards, dtype=np.float32), \\\n","           np.array(dones, dtype=np.uint8), np.array(last_states, copy=False)\n","\n","class NoisyLinear(nn.Linear):\n","    def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):\n","        super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)\n","        self.sigma_weight = nn.Parameter(torch.full((out_features, in_features), sigma_init))\n","        self.register_buffer(\"epsilon_weight\", torch.zeros(out_features, in_features))\n","        if bias:\n","            self.sigma_bias = nn.Parameter(torch.full((out_features,), sigma_init))\n","            self.register_buffer(\"epsilon_bias\", torch.zeros(out_features))\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        std = math.sqrt(3 / self.in_features)\n","        self.weight.data.uniform_(-std, std)\n","        self.bias.data.uniform_(-std, std)\n","\n","    def forward(self, input):\n","        self.epsilon_weight.normal_()\n","        bias = self.bias\n","        if bias is not None:\n","            self.epsilon_bias.normal_()\n","            bias = bias + self.sigma_bias * self.epsilon_bias.data\n","        return F.linear(input, self.weight + self.sigma_weight * self.epsilon_weight.data, bias)\n","      \n","################################################################################\n","\n","class RainbowDQN(nn.Module):\n","  def __init__(self, input_shape, n_actions):\n","    super(RainbowDQN, self).__init__()\n","    \n","    self.conv = nn.Sequential(\n","        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n","        nn.ReLU()\n","        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","        nn.ReLU(),\n","        nn.Conv2d(64,64, kernel_size=3, stride=1),\n","        nn.ReLU()\n","    )\n","    \n","    conv_out_size = self._get_conv_out(input_shape)\n","    self.fc_val = nn.Sequential(\n","        NosiyLinear(conv_out_size, 512),\n","        nn.ReLU()\n","        NoisyLinear(512, n_actions*N_ATOMS)\n","    )\n","    \n","    self.fc_adv = nn.Sequential(\n","        NoisyLinear(conv_out_size, 512),\n","        nn.ReLU(),\n","        NoisyLinear(512, n_actions*N_ATOMS)\n","    )\n","    \n","    self.register_buffer(\"supports\", torch.arange(Vmin, Vmax+DELTA_Z, DELTA_Z))\n","    self.softmax = nn.Softmax(dim=1)\n","    \n","  def _get_conv_out(self, shape):\n","    o = self.conv(torch.zeros(1, *shape))\n","    return int(np.prod(o.size()))\n","  \n","  def forward(self, x):\n","    batch_size = x.size()[0]\n","    fx = x.float()/256\n","    conv_out = self.conv(fx).view(batch_size, -1)\n","    val_out = self.fc_val(conv_out). view(batch_size, 1, N_ATOMS)\n","    adv_out = self.fc_adv(conv_out).view(batch_size, -1, N_ATOMS)\n","    adv_mean = adv_out.mean(dim=1, keepdim=True)\n","    return val_out+adv_out-adv_mean\n","  \n","  def both(self, x):\n","    cat_out = self(x)\n","    probs = self.apply_softmax(cat_out)\n","    weights = probs*self.supports\n","    res = weights.sum(dim=2)\n","    return cat_out, res\n","  \n","  def qvals(self,x):\n","    return self.both(x)[1]\n","  \n","  \"\"\"\n","  The prceding functions are used to be able to combine probability distributions into Q-values \n","  without calling the network several times\n","  \"\"\"\n","def calc_los(batch, batch-weights, net, tgt_net, gamma, device = \"cuda\"):\n","  states, actions, rewards, dones, next_states = unpack_batch(batch)\n","  \n","  batch_size = len(batch)\n","  \n","  states_v = torch.tensor(states).to(device)\n","  actions_v = torch.tensor(actions).to(device)\n","  next_states_v = torch.tensor(next_states).to(device)\n","  \n","  batch_weights_v = torch.tensor(batch_weights).to(device)\n","  \n","  \"\"\"\n","  Our loss function accepts the same set of arguments as we've seen in prioritized replay buffer. In addition \n","  to batch array with training data, we pass weights for every sample\n","  \"\"\"\n","  \n","  distr_v, qvals_v = net.both(torch.cat((states_v, nest_states_v)))\n","  next_qvals_v = qvals_v[batch_size:]\n","  distr_v = distr_v[:batch_size]\n","  \n","  \"\"\"\n","  Here we use a small trick to speed up calculations a bit. As the double DQN method requires us to use our main\n","  network to select actions but use the target network to obtain values (in our case, value distributions) for \n","  those actions, we need to pass to our main network both the current states and the next states.\n","  Earlier, we calculated the network output in two calls, which is not very efficient on GPU. Now, we concatenate \n","  both current states and next states into one tensor and obtain the result in one network pass, splitting the \n","  result later. We need to calculate both Q-values and raw values distributions, as our action selection policy is\n","  still greedy: we choose the action with the largest q value\n","\n","  \"\"\"\n","  next_actions_v = next_qvals_v.max(1)[1]\n","  next_distr_v = tgt_net(next_states_v)\n","  next_best_distr_v = next_distr_v[range(batch_size), next_actions_v.data]\n","  next_best_distr_v = tgt_net.apply_softmax(next_best_distr_v)\n","  next_best_distr = next_best_distr_v.data.cpu().numpy()\n","  \"\"\"\n","  In the prceding lines, we decide on actions to take in the next state and obtain the distribution of those actions\n","  using our target network. So, the above ne/tgt-net suffling implements the double dqn method. Then we apply s\n","  softmax to distribution for those best actions and copy the data into cpu to perform the bellman projection\n","  \n","  \"\"\"\n","  dones = dones.astype(np.bool)\n","  proj_distr = distr_projection(next_best_distr, rewards, dones, Vmin, VVmax, N_ATOMS, gamma)\n","  \n","  \"\"\"\n","  In the preceding code, we calculate the projected distribution using the bellman equation. This rsult will be \n","  used as a target in our KL-divergence\n","  \"\"\"\n","  state_action_values = distr_v[range(batch_size), actions_v.data]\n","  state_log_sm_v = F.log_softmax(state_action-values, dim=1)\n","  \"\"\"\n","  here we obtain the distributions for taken actions and apply log_softmax to calculate the loss\n","  \"\"\"\n","  proj_distr_v = torch_tensor(proj_distr)\n","  loss_v = -state_log_sm_v*proj_distr_v\n","  loss_v = batch_weights_v*loss_v.sum(dim=1)\n","  return loss_v.mean(), loss_v+1e-5\n","\"\"\"\n","In the last lines of the function we calculate the KL-divergence loss, multiply it by weights and return two quantities\n","combined loss to be used in the optimizer and individual loss values for batch, which will be used as priorities in the \n","replay buffer. The rest contains initialization and training loop\n","\"\"\"\n","\n","if __name__ ==\"__main__\":\n","  params = HYPERPARAMS['pong']\n","  device = torch.device(\"cuda\")\n","  \n","  env = gym.make(params['env_name'])\n","  env = ptan.common.wrappers.wrap_dqn(env)\n","  \n","  net = RainbowDQN(env.observation_space.shape, env.action_space.n).to(device)\n","  tgt_net = ptan.agent.TargetNet(net)\n","  agent = ptan.agent.DQNAgent(lambda x: net.qvals(x), ptan.actions.ArgmaxActionSelector(), device=device)\n","  \n","  exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=REWARD_STEPS)\n","  buffer = ptan.experience.PrioritizedReplayBuffer(exp_source, params['replay_size'], PRIO_REPLAY_ALPHA)\n","  optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n","  \n","  frame_idx = 0\n","  beta = BETA_START\n","  with RewarTracker(writer, params['stop_reward']) as reward_tracker:\n","    while True:\n","      frame_idx+=1\n","      buffer.populate(1)\n","      beta = min(1.0, BETA_START+frame_idx*(1.0-BETA_START)/BETA_FRAMES)\n","      \n","      new_rewards = exp_source.pop_total_rewards()\n","      if new_rewards:\n","        if reward_tracker.reward(new_rewards[0], frame_idx):\n","          break\n","      if len(buffer) < params['replay_initial']:\n","        continue\n","      optimizer.zero_grad()\n","      batch, batch_indicies, batch_weights = buffer.sample(params['batch_size'], beta)\n","      loss_v, sample_prios_v = calc_loss(batch, batch_weights, net, target_net.target_model, params['gamma']**REWARD_STEPS,\n","                                        device=device)\n","      loss_v.backward()\n","      optimizer.step()\n","      buffer.update_priorities(batch_indicies, sample_prios_v.data.cpu().numpy())\n","      if frame_idx%params['target_net_sync']==0:\n","        tgt_net.sync()\n","                        \n","  "],"execution_count":0,"outputs":[]}]}