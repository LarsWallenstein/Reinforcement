{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A3C on pong","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wLj-Z9IVz1Ea","colab_type":"text"},"source":["#Common methods"]},{"cell_type":"code","metadata":{"id":"6PXwcEnOzQn7","colab_type":"code","colab":{}},"source":["import sys\n","import time\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","class AtariA2C(nn.Module):\n","    def __init__(self, input_shape, n_actions):\n","        super(AtariA2C, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n","            nn.ReLU()\n","        )\n","\n","        conv_out_size = self._get_conv_out(input_shape)\n","        self.policy = nn.Sequential(\n","            nn.Linear(conv_out_size, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, n_actions)\n","        )\n","\n","        self.value = nn.Sequential(\n","            nn.Linear(conv_out_size, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 1)\n","        )\n","\n","    def _get_conv_out(self, shape):\n","        o = self.conv(torch.zeros(1, *shape))\n","        return int(np.prod(o.size()))\n","\n","    def forward(self, x):\n","        fx = x.float() / 256\n","        conv_out = self.conv(fx).view(fx.size()[0], -1)\n","        return self.policy(conv_out), self.value(conv_out)\n","\n","\n","class RewardTracker:\n","    def __init__(self, writer, stop_reward):\n","        self.writer = writer\n","        self.stop_reward = stop_reward\n","\n","    def __enter__(self):\n","        self.ts = time.time()\n","        self.ts_frame = 0\n","        self.total_rewards = []\n","        return self\n","\n","    def __exit__(self, *args):\n","        self.writer.close()\n","\n","    def reward(self, reward, frame, epsilon=None):\n","        self.total_rewards.append(reward)\n","        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n","        self.ts_frame = frame\n","        self.ts = time.time()\n","        mean_reward = np.mean(self.total_rewards[-100:])\n","        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n","        print(\"%d: done %d games, mean reward %.3f, speed %.2f f/s%s\" % (\n","            frame, len(self.total_rewards), mean_reward, speed, epsilon_str\n","        ))\n","        sys.stdout.flush()\n","        if epsilon is not None:\n","            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n","        self.writer.add_scalar(\"speed\", speed, frame)\n","        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n","        self.writer.add_scalar(\"reward\", reward, frame)\n","        if mean_reward > self.stop_reward:\n","            print(\"Solved in %d frames!\" % frame)\n","            return True\n","        return False\n","\n","\n","def unpack_batch(batch, net, last_val_gamma, device='cpu'):\n","    \"\"\"\n","    Convert batch into training tensors\n","    :param batch:\n","    :param net:\n","    :return: states variable, actions tensor, reference values variable\n","    \"\"\"\n","    states = []\n","    actions = []\n","    rewards = []\n","    not_done_idx = []\n","    last_states = []\n","    for idx, exp in enumerate(batch):\n","        states.append(np.array(exp.state, copy=False))\n","        actions.append(int(exp.action))\n","        rewards.append(exp.reward)\n","        if exp.last_state is not None:\n","            not_done_idx.append(idx)\n","            last_states.append(np.array(exp.last_state, copy=False))\n","    states_v = torch.FloatTensor(states).to(device)\n","    actions_t = torch.LongTensor(actions).to(device)\n","\n","    # handle rewards\n","    rewards_np = np.array(rewards, dtype=np.float32)\n","    if not_done_idx:\n","        last_states_v = torch.FloatTensor(last_states).to(device)\n","        last_vals_v = net(last_states_v)[1]\n","        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0]\n","        rewards_np[not_done_idx] += last_val_gamma * last_vals_np\n","\n","    ref_vals_v = torch.FloatTensor(rewards_np).to(device)\n","    return states_v, actions_t, ref_vals_v"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ststl78kz7Wu","colab_type":"text"},"source":["#Data parallelism"]},{"cell_type":"markdown","metadata":{"id":"sGv508e_0jHx","colab_type":"text"},"source":["###Imports"]},{"cell_type":"code","metadata":{"id":"SlSfdQjH0CTh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"3ceacda9-b9ef-47e3-de7a-195d2b647ee3","executionInfo":{"status":"ok","timestamp":1566049442824,"user_tz":-180,"elapsed":11317,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["!pip install ptan\n","import gym \n","import ptan\n","import numpy as np\n","import collections\n","\n","import torch.nn.utils as nn_utils\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.multiprocessing as mp"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting ptan\n","  Downloading https://files.pythonhosted.org/packages/41/bc/79b901be607ae861ca24d1ba504ced953c21be05edbd3518a3ff11610932/ptan-0.4.tar.gz\n","Building wheels for collected packages: ptan\n","  Building wheel for ptan (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ptan: filename=ptan-0.4-cp36-none-any.whl size=21534 sha256=59ca988181ea5129db0812ce4ddffd47cd0fede95627ee77568732a80bbe5048\n","  Stored in directory: /root/.cache/pip/wheels/f8/21/fa/ad8d37fd306e72310c8b9b0e24a1bfec36c8587b1721d5c63d\n","Successfully built ptan\n","Installing collected packages: ptan\n","Successfully installed ptan-0.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dPO7Bkso0ms-","colab_type":"text"},"source":["###Global variables"]},{"cell_type":"code","metadata":{"id":"vJEPYW5N0d7e","colab_type":"code","colab":{}},"source":["GAMMA = 0.99\n","LEARNING_RATE = 0.001\n","ENTROPY_BETA = 0.01\n","BATCH_SIZE = 128\n","\n","REWARD_STEPS = 4\n","CLIP_GRAD = 0.1\n","\n","PROCESSES_COUNT = 4 #Number of children threads equals to number of cores\n","NUM_ENVS = 15 # the number of envs every child process will use to gather data\n","# So the total number of envs we're working with at the sam time is 4*15 = 60\n","\n","ENV_NAME = \"PongNoFrameskip-v4\"\n","NAME = 'pong'\n","REWARD_BOUND=18"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hu1tznip2KSv","colab_type":"text"},"source":["###Wrappers for the child processes"]},{"cell_type":"code","metadata":{"id":"lcqv4oy_2Qen","colab_type":"code","colab":{}},"source":["def make_env():\n","  return ptan.common.wrappers.wrap_dqn(gym.make(ENV_NAME))\n","\n","#_________data_function_________________________________________________________\n","def data_func(net, device, train_queue):\n","  TotalReward = collections.namedtuple('TotalReward', field_names = 'reward')\n","  envs = [make_env() for _ in range(NUM_ENVS)]\n","  agent = ptna.agent.PolicyAgent(lambda x: net(x)[0], device = device, apply_softmax=True)\n","  \n","  exp_source = ptan.experience.ExperienceSourceFirstLast(envs, agent, gamma, steps_count=REWARD_STEPS)\n","  for exp in exp_source:\n","    new_rewards= exp_source.pop_total_rewards()\n","    if new_rewards:\n","      train_queue.put(TotalReward(reward=np.mean(new_rewards)))\n","    train_queue.put(exp)\n","#_______________________________________________________________________________"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d8lO-LYW4llA","colab_type":"text"},"source":["$$Data-function$$\n","This function is executed in the child process. We pass it three arguments: our NN, the device to be used to perform computation and the queue we'll use to send data from the child process to our master procss, which will perform training. The queue is used in many-producers and one-consumer mode and can contain two different types of objects:\n","\n","1. **TotalReward:**  This is a preceding object tat we've defined, which has only one field reward, which is a float value of the total discounted reward for the completed episode\n","\n","2. **ExperienceFirstLast:**  is an object that wraps the first state in the subsequence of REWARD_STEPS, action taken , the discounted reward for this subsequnce and the last state. This is our experience that we'll use for training"]},{"cell_type":"markdown","metadata":{"id":"joeXE_tU7fKs","colab_type":"text"},"source":["###Main process"]},{"cell_type":"code","metadata":{"id":"mc_jlXhb7Vly","colab_type":"code","colab":{}},"source":["if __name__ = '__main__':\n","  #______mp__________________\n","  mp.set_start_method('spawn')\n","  #___________________________\n","  device = \"cuda\"\n","  #writer = SummaryWriter(comment = \"-a3c-data\"+NAME+\"_\"+args.name)\n","  \n","  #_______weights_sharing___________________________________________________\n","  env = make_env()\n","  net = AtariA2C(env.observation_space.shape, env.action_space.n).to(device)\n","  net.share_memory()\n","  optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n","  #_________________________________________________________________________\n","  \n","  #_______Child_processes_start_____________________________________________\n","  train_queue = mp.Queue(maxsize=PROCESSES_COUNT)\n","  data_proc_list=[]\n","  for _ in range(PROCESSES_COUNT):\n","    data_proc = mp.Process(target=data_func, args=(net, device, train_queue))\n","    data_proc.start()\n","    data_proc_list.append(data_proc)\n","  #_________________________________________________________________________\n","  \n","  batch =[]\n","  step_idx = 0\n","  \n","  try:\n","    with RewardTracker(writer, stop_reward =REWARD_BOUND)as tracker:\n","      with ptan.common.utils.TBMeanTracker(writer, batch_size=100)as tb_tracker:\n","        #____1_____________________________________________\n","        while True:\n","          train_entry = train_queue.get()\n","          if isinstance(train_entry, TotalReward):\n","            if tracker.reward(train_entry.reward, step_idx):\n","              break\n","            continue\n","          step_idx+=1\n","          batch.append(train_entry)\n","          if len(batch)<BATCH_SIZE:\n","            continue\n","          #________________________________________________\n","          \n","          #_______BATCH_________________________________________________________________________________________________\n","          sates_v, actions_t, vals_ref_v = unpack_batch(batch, net, last_val_gamma=GAMMA**REWARD_STEPS, device = device)\n","          batch.clear()\n","          #_____________________________________________________________________________________________________________\n","          \n","          optimizer.zero_grad()\n","          logits_v, value_v = net(states_v)\n","          \n","          loss_value_v = F.mse_loss(value_v.squeze(-1), vals_ref_v)\n","          \n","          log_prob_v = F.log_softmax(logits_v, dim=1)\n","          adv_v = vals_ref_v-value_v.detach()\n","          log_prob_actions_v = adv_v*log_prob_v[range(BATCH_SIZE), actions_t]\n","          loss_policy_v = - log_prob_actions_v.mean()\n","          \n","          prob_v = F.softmax(logits_v, dim=1)\n","          \n","          entropy_loss_v = ENTROPY_BETA*(prob_v*log_prob_v).sum(dim=1).mean()\n","          \n","          loss_v = entropy_loss_v+loss_value_v+loss_policy_v\n","          loss_v.backward()\n","          \n","          nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n","          optimizer.step()\n","          \n","          tb_tracker.track(\"advantage\", adv_v, step_idx)\n","          tb_tracker.track(\"values\", value_v, step_idx)\n","          tb_tracker.track(\"batch_rewards\", vals_ref_v, step_idx)\n","          tb_tracker.track(\"loss_entropy\", entropy_loss_v, step_idx)\n","          tb_tracker.track(\"loss_policy\", loss_policy_v, step_idx)\n","          tb_tracker.track(\"loss_value\", loss_value_v, step_idx)\n","          \n","          tb_tracker.track(\"loss_total\", loss_v, step_idx)\n","        finally:\n","          for p in data_proc_list:\n","            \n","            p.terinate()\n","            p.join()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G4Rft1VV8hcK","colab_type":"text"},"source":["$$MP$$\n","This method instructs the multiprocessing module about the kind of parallelism we want to us. The native multiprocessing library in pthon supprts several ways to support subprocesses, but due to PyTorch multiprocessinglimitations, spawn is the best option\n","\n","$$Weights-Sharing$$\n","\n","After we create out NN, move it to the CUDA device and ask it to share its weights. CUDA tensors are shared by Default, but for CPU mode, a call to share_memory is required\n","\n","$$Child-Processes-Start$$\n","Firstly we create the queue that will be used by child processes to deliver to us data. The argument to the queue constructor specifies the maximum queue capacity. All attempts to push a new item to the full queue will be blocked, which is very convinient for us to keepour data samples on-policy. After the queue creation, we start the required amount of processes using the mp.Process class and keep them for correct shutdown in a list. Right after the mp.Process.start() call, our data_func function will be executedby the child process\n","\n","$$1$$\n","In the begining of the training loop, we get the next entry fom the queue and handle possible total reward objects, which we pass to the reward tracker.\n","\n","As we can have only two types of objects in the queue (TotalReward and experience transitions), we need to check entry obtained from the queue only once. After the TotalReward entries are handled, we put experience objects into the batch accumulated until the required batch size\n","\n","$$BATCH$$\n","As we get the required amount of experience samples, we convert them into training data using the unpack_batch function and clear the batch. One thing to note: as our experience samples represent four stepssubsequences (as reward_steps is four) we need to use a proper discount factor of $\\gamma^4$ for the last $V(s)$ reward term. The rest of the training loop is standart actor critic loss calculation, which is performed in exactly the same way as in previous chapter: we calculate the logits of the policy and value estimation using our current network and calculate policy, value and entropy losses"]},{"cell_type":"markdown","metadata":{"id":"iwkd64l1ID1L","colab_type":"text"},"source":["#Gradients Parallelism"]},{"cell_type":"markdown","metadata":{"id":"MkXW96Q2IOj_","colab_type":"text"},"source":["###Additional global variables"]},{"cell_type":"code","metadata":{"id":"aA6lAmda8-cy","colab_type":"code","colab":{}},"source":["GARD_BATCH = 64\n","TRAIN_BATCH=2\n","\n","\"\"\"\n","BATCH_SIZE is replaced by two parameters\n","\n","  1. GRAD_BATCH defines the size of the batch used by every child process to compute the loss and get the value \n","  of gradients\n","  \n","  2. TRAIN_BATCH specifies how much gradient batches from the child processes will be combines on every SGD \n","  iteration. Every entry produced by the child process has the same shape as our network parameters and we sum up \n","  TRAIN_BATCH values of them together. So for every optimizaation step, we use th TRAIN_BATCH*GRAD_BATCH training \n","  samples As the loss calculation and backprop are quite heavy operations, we use large GRAD_BATCH to make them more \n","  efficient. Due to this large batch, we should keep TRAIN_BATCH relatively low to keep our network update \n","  on_policy\n","  \n","\"\"\"\n","\n","def grads_func_2(proc_name, net, device, train_queue):\n","  envs = [make_env() for _ in range(NUM_NUMS)]\n","  \n","  agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], device=device, apply_softmax=True)\n","  exp_source = ptan.seperience.ExperienceSourceFirstLast(envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n","  \n","  batch = []\n","  frame_idx = 0\n","  writer = SummaryWriter(comment=proc_name)\n","  \n","  with RewardTracker(writer, stop_reward=REWARD_BOUND) as tracker:\n","    with ptan.common.utils.TBMeanTracker(writer, batch_size=100) as tb_tracker:\n","      for exp in exp_source:\n","        frame_idx+=1\n","        new_rewards =exp_source.pop_total_rewards()\n","        if new_rewards and tracker.reward(new_rewards[0], frame_idx):\n","          \n","          break\n","        batch.append(exp)\n","        if len(batch)<GRAD_BATCH:\n","          continue\n","        #Up to this point we've gathered the batch with transitions and handled the end of the episode rewards\n","        \n","        states_v, actions_t, vals_ref_v = unpack_batch(batch, net, last_val_gamma=GAMMA**REWARD_STEPS, device=device)\n","        batch.clear()\n","        \n","        net.zero_grad()\n","        logits_v, value_v = net(states_v)\n","        loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)\n","        log_prob_v = F.log_softmax(logits_v, dim=1)\n","        adv_v = vals_ref_v-value_v.detach()\n","        log_prob_actions_v = adv_v*log_prob_v[range(GRAD_BATCH), actions_t]\n","        loss_policy_v = -log_prob_actions_v.mean()\n","        \n","        prob_v = F.softmax(logits_v, dim=1)\n","        entropy_loss_v = ENTROPY_BETA*(prob_v*log_prob_v).sum(dim=1).mean()\n","        loss_v = entropy_loss_v+loss_value_v+loss_policy_v\n","        loss_v.backward()\n","        \n","        \n","        tb_tracker.track(\"advantage\", adv_v, frame_idx)\n","        tb_tracker.track(\"values\", value_v, frame_idx)\n","        tb_tracker.track(\"batch_rewards\", vals_ref_v, frame_idx)\n","        tb_tracker.track(\"loss_entropy\", entropy_loss_v, frame_idx)\n","        tb_tracker.track(\"loss_policy\", loss_policy_v, frame_idx)\n","        tb_tracker.track(\"loss_value\", loss_value_v, frame_idx)\n","        tb_tracker.track(\"loss_total\", loss_v, frame_idx)\n","        \n","        np.utils.clip_grad_norm(net.parameters(), CLIP_GRAD)\n","        grads = [param.grad.data.cpu().numpy() if\n","                param.grad is not None\n","                else None for param in net.parameters()]\n","        train_queue.put(grads)\n","  train_queue.put(None)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqbr78HKO0Vt","colab_type":"code","colab":{}},"source":["if __name__ = \"__main__\":\n","  mp.set_start_method('spawn')\n","  device=\"cuda\"\n","  \n","  env = make_env()\n","  net = AtariA2c(env.observation_space.shape, env.action_space.n).to(device)\n","  net.share_memory()\n","  \n","  optimizer = optim.Adam(net.parameters(), lr = LEARNING_RATE, eps=1e-3)\n","  \n","  train_queue = mp.Queue(maxsize=PROCESSES_COUNT)\n","  data_proc_list=[]\n","  for proc_idx in range(PROCESSES_COUNT):\n","    proc_name = \"-a3c-grad_\" + NAME + \"_\" + args.name + \"#%d\" % proc_idx\n","    data_proc=mp.Process(target=grads_func, args=(proc_name, net, device, train_queue))\n","    data_proc.start()\n","    data_proc_list.append(data_proc)\n","  batch=[]\n","  step_idx = 0\n","  grad_buffer = None\n","  \n","  try:\n","    while True:\n","      train_entry = train_queue.get()\n","      if train_entry is None:\n","        break\n","      step_idx==1\n","      \n","      if grad_buffer is None:\n","        grad_buffer = train_entry\n","      else:\n","        for tgt_grad, grad in zip(grad_buffer, train_entry):\n","          tgt_grad==grad\n","          \n","      if step_idx % TRAIN_BATCH==0:\n","        for param, grad in zip(net.parameters(), grad_buffer):\n","          grad_v = torch.FloatTensor(grad).to(device)\n","          param.grad = grad_v\n","          \n","      nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n","      optimizer.step()\n","      grad_buffer = None\n","  finally:\n","    for p in data_proc_list:\n","      p..terminate()\n","      p.join()"],"execution_count":0,"outputs":[]}]}